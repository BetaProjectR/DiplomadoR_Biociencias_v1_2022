---
title: "Clase 16 Regresión lineal múltiple y supuestos."
author: Dr. José Gallardo Matus & Dra. Angélica Rueda Calderón
institute: Pontificia Universidad Católica de Valparaíso
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  beamer_presentation:
    theme: "Malmoe"
    colortheme: "beaver"
    fonttheme: "structurebold"
    includes:
            in_header: mystyle.tex
subtitle: 'Diplomado en Análisis de Datos con R e Investigación reproducible para Biociencias.'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(readxl)
library(dplyr)
library(ggplot2)
library(knitr)
library(car)
library(lmtest)
library(psych)
library(agridat)
library(gridExtra)
```

# PLAN DE LA CLASE

**1.- Introducción**
    
- Modelo de regresión lineal múltiple (MRLM).
- Estudio de caso: transformación de variables predictoras.
- Pruebas de hipótesis.
- Supuestos de MRLM
- El problema de la multicolinealidad
- ¿Cómo seleccionar variables?
- ¿Cómo comparar modelos?
- Interpretación regresión lineal múltiple con R.

**2.- Práctica con R y Rstudio cloud.** 

- Realizar análisis de regresión lineal múltiple.  
- Realizar gráficas avanzadas con ggplot2.  
- Elaborar un reporte dinámico en formato html.  
  

# REGRESIÓN LINEAL MÚLTIPLE

Sea $Y$ una variable respuesta continua y $X_1,…,X_p$ variables predictoras, un modelo de regresión lineal múltiple se puede representar como,
 
$$Y_{i} = \beta_{0} + \beta_{1} X_{i1} + \beta_{2} X_{i2} + ... + \beta_{p} X_{ip} + \epsilon_{i}$$

$\beta_{0}$ = Intercepto.
$\beta_{1}, \beta_{2},..., \beta_{p}$ = Coeficientes de regresión.


# ESTUDIO DE RENDIMIENTO EN MAÍZ  

En este estudio de caso trabajaremos con un subset de datos de maíz (*corn*) del paquete de R **agridat** (n=162). 

heady.fertilizer {agridat}

| **Variable** | **Descripción**            | **Tipo de efecto/ variable**                                 |
|--------------|----------------------------|--------------------------------------------------------------|
| **Crop**     | Cultivo se seleccionó maíz | Criterio de clasificación/ Factor de tratamiento cualitativo |
| **P**        | Mediciones de fósforo      | Variable regresora númerica                                  |
| **N**        | Mediciones de nitrógeno    | Variable regresora númerica                                  |
| **yield**    | Rendimiento (g).           | Variable respuesta/ Cuantitativa continua                    |

 
```{r, message=FALSE, out.width = '80%', fig.align='center'}
data(heady.fertilizer)
dat <- heady.fertilizer

d1 <- subset(dat, crop=="corn")
d2 <- d1[,c(3,5,6)]

knitr::kable(head(d1[,c(1,3,5,6)]),caption ="Tabla de datos de maíz")
```


[heady.fertilizer](https://cran.rstudio.com/web/packages/agridat/agridat.pdf)

# PRUEBAS DE HIPÓTESIS REGRESIÓN LINEAL MÚLTIPLE

- **Intercepto**  
Igual que en regresión lineal simple.
   
- **Modelo completo**  
Igual que en regresión lineal simple.
   
- **Coeficientes**  
Uno para cada variable y para cada factor de una variable de clasificación.  


# ANÁLISIS DE REGRESIÓN LINEAL MÚLTIPLE: PROBLEMAS

Para *p* variables predictoras existen *N* modelos diferentes que pueden usarse para estimar, modelar o predecir la variable respuesta.

**Problemas**  
- ¿Qué hacer si las variables predictoras están correlacionadas?.  
- ¿Cómo seleccionar variables para incluir en el modelo?.  
- ¿Qué hacemos con las variables que no tienen efecto sobre la variable respuesta?.  
- Dado N modelos ¿Cómo compararlos?, ¿Cuál es mejor?. 


# SUPUESTOS DE MODELO DE REGRESIÓN LÍNEAL MÚLTIPLE

¿Cuales son los supuestos?  
- Independencia.  
- Linealidad entre variable cada variable independiente y dependiente.  
- Homocedasticidad.  
- Normalidad.  
- Multicolinealidad.


# MODELO DE REGRESIÓN LÍNEAL MÚLTIPLE

```{r, echo=TRUE, out.width = '75%', message=FALSE, fig.align='center'}
# Crea modelo de regresión múltiple con lm()
m1 <- lm(yield ~ N + P + sqrt(N) + sqrt(P) + sqrt(N*P), 
         data=d2)
# Imprime resultado con función summary()
summary(m1)$coefficients%>%kable()
```

$R^2$ = `r  round(summary(m1)$r.squared,2)`, *p-val* = `r anova(m1)$'Pr(>F)'[1]`


# SUPUESTO DE INDEPENDENCIA: **PRUEBA DE DURBIN-WATSON**

**H~0~**: Los residuos son independientes entre sí. 

```{r, echo= TRUE}
dwtest(yield ~ N + P + sqrt(N) + sqrt(P) + sqrt(N*P), 
       data=d2,
       alternative = c("two.sided"), 
       iterations = 15)
```

# SUPUESTO DE LINEALIDAD: **MÉTODO GRÁFICO**

**H~0~**: Hay relación lineal entre cada variable predictora y la variable respuesta.

```{r, echo=FALSE, out.width = '75%', message=FALSE, fig.align='center', error=FALSE, warning=FALSE}
plot1 <- ggplot(data = d2, aes(x = N, y = yield)) +
  geom_point(position = position_jitter(w = 0, h = 0.1) ) +
  labs(x = "Nitrógeno", y = "Rendimiento") +
  scale_shape_manual(values=c(1,2)) +
  stat_smooth(method='loess',formula=y~x, se=T)+
  scale_color_brewer(palette="Set1") + 
  theme(legend.position="none") +
  theme(panel.border=element_blank(), axis.line=element_line())

plot2  <- ggplot(data = d2, aes(x = P, y = yield)) +
  geom_point(position = position_jitter(w = 0, h = 0.1) ) +
  labs(x = "Fósforo", y = "Rendimiento") +
  scale_shape_manual(values=c(1,2)) +
  stat_smooth(method='loess',formula=y~x, se=T)+
  scale_color_brewer(palette="Set1") + 
  theme(legend.position="none") +
  theme(panel.border=element_blank(), axis.line=element_line())

grid.arrange(plot1, plot2, ncol=2, nrow =1)
```


# SUPUESTO DE HOMOGENEIDAD DE VARIANZAS: **PRUEBA DE BREUSCH-PAGAN**

**H~0~**: La varianza de los residuos es constante.

```{r, echo=TRUE}
bptest(m1)
```


# SUPUESTO DE **MULTICOLINEALIDAD**

Correlaciones >0,80 es problema. 

```{r, out.width = '75%', message=FALSE, fig.align='center'}
pairs.panels(d2)
```

# FACTOR DE INFLACIÓN DE LA VARIANZA (VIF).

- **VIF ** es una medida del grado en que la varianza del estimador de mínimos cuadrados incrementa por la colinealidad entre las variables predictoras.
- Mayor a 10 es evidencia de alta multicolinealidad.

```{r, echo=TRUE}
m1 <- lm(yield ~ N + P + sqrt(N) + sqrt(P) + sqrt(N*P), 
         data=d2)
vif(m1) %>% kable(digits=2, col.names = c("VIF"))

```

# ¿CÓMO RESOLVEMOS MULTICOLINEALIDAD?

- Eliminar variables correlacionadas, pero podríamos eliminar una variable causal.
    

- Transformar una de las variables: log u otra.
  

- Reemplazar por variables ortogonales: Una solución simple y elegante son los componentes principales (ACP).

# DATOS INFLUYENTES

influencePlot(m1)

```{r, echo=FALSE, out.width = '90%', out.height= '70%', fig.align='center'}
knitr::include_graphics("Plot_influyent.png")
```

# SUPUESTO DE NORMALIDAD: **PRUEBA DE SHAPIRO-WILKS**

**H~0~**: Los residuos tienen distribución normal.  

```{r, echo=TRUE}
shapiro.test(x= rstudent(m1)) 
shapiro.test(x= rstudent(m1)[-90])
```

- Eliminar el dato influyente (observación 90 y volver a correr el modelo)

# COMPARACIÓN: MODELO COMPLETO 0

```{r, echo=TRUE}
# Crea modelo de regresión múltiple
lm0<- lm(yield ~ N + P + sqrt(N) + sqrt(P) + sqrt(N*P), 
         data=d2)
```

```{r, echo=FALSE}
# Imprime resultado de modelo de regresión múltiple
summary(lm0)$coef %>% kable()
```

$R^2$ = `r  round(summary(lm0)$r.squared,2)`, *p-val* = `r anova(lm0)$'Pr(>F)'[1]`


# COMPARACIÓN: MODELO REDUCIDO 1

```{r, echo=TRUE}
lm1<- lm(yield ~ N + P + N:P , 
         data=d2)
```

```{r, echo=FALSE}
summary(lm1)$coef %>% kable()
```
$R^2$ = `r  round(summary(lm1)$r.squared,2)`, *p-val* = `r anova(lm1)$'Pr(>F)'[1]`

# COMPARACIÓN: MODELO REDUCIDO 2

```{r, echo=TRUE}
lm2<- lm(yield ~ sqrt(N) + sqrt(P) + sqrt(N*P), 
         data=d2)
```

```{r, echo=FALSE}
summary(lm2)$coef %>% kable()
```

$R^2$ = `r  round(summary(lm2)$r.squared,2)`, *p-val* = `r anova(lm2)$'Pr(>F)'[1]`

# CRITERIOS PARA COMPARAR MODELOS

Existen diferentes criterios para comparar modelos.

- Anova de residuales (RSS).  
- Criterios que penalizan incrementar el número de parámetros estimados (más variables predictoras):  
  a) Akaike Information Criterion (AIC).  
  b) Bayesian Information Criterion (BIC).  
- En todos los casos mientras menor es el valor de RSS, AIC o BIC mejor es el modelo.
- No necesariamente los resultados son equivalentes entre criterios.

# COMPARACIÓN USANDO RESIDUALES

```{r, echo=TRUE}
anova(lm0, lm1, lm2) %>% kable()
```

# COMPARACIÓN USANDO AIC Y BIC

**AIC** = $-2*log-likelihood + 2*K$  
**BIC** = $-2*log-likelihood + log(n)*K$  
**K**= número de parámetros a estimar.  

```{r, echo=FALSE}
# Compara modelos usando AIC
AIC(lm0, lm1, lm2) %>% kable()
```


```{r, echo=FALSE}
# Compara modelos usando BIC
BIC(lm0, lm1, lm2) %>% kable()
```


# PRÁCTICA ANÁLISIS DE DATOS

- El trabajo práctico se realiza en Rstudio.cloud.

**Guía 16 Regresión lineal multiple.**

# RESUMEN DE LA CLASE

- Elaborar hipótesis para una regresión lineal múltiple.

- Interpretar coeficientes.

- Evaluar supuestos.

- Comparar modelos: residuales, AIC, BIC.
